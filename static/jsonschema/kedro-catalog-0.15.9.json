{
  "type": "object",
  "patternProperties": {
    "^[a-z0-9-_]+$": {
      "required": ["type"],
      "properties": {
        "type": {
          "type": "string",
          "enum": [
            "networkx.NetworkXDataSet",
            "dask.ParquetDataSet",
            "biosequence.BioSequenceDataSet",
            "matplotlib.MatplotlibWriter",
            "yaml.YAMLDataSet",
            "pickle.PickleDataSet",
            "text.TextDataSet",
            "spark.SparkJDBCDataSet",
            "spark.SparkHiveDataSet",
            "spark.SparkDataSet",
            "pandas.JSONBlobDataSet",
            "pandas.JSONDataSet",
            "pandas.SQLTableDataSet",
            "pandas.SQLQueryDataSet",
            "pandas.ParquetDataSet",
            "pandas.FeatherDataSet",
            "pandas.CSVBlobDataSet",
            "pandas.HDFDataSet",
            "pandas.CSVDataSet",
            "pandas.ExcelDataSet",
            "pandas.GBQTableDataSet",
            "PickleLocalDataSet",
            "JSONLocalDataSet",
            "HDFLocalDataSet",
            "PartitionedDataSet",
            "CachedDataSet",
            "JSONDataSet",
            "CSVHTTPDataSet",
            "MemoryDataSet",
            "CSVLocalDataSet",
            "ExcelLocalDataSet",
            "LambdaDataSet",
            "HDFS3DataSet",
            "PickleS3DataSet",
            "SQLTableDataSet",
            "SQLQueryDataSet",
            "CSVS3DataSet",
            "ParquetLocalDataSet",
            "TextLocalDataSet"
          ]
        }
      },
      "allOf": [
        {
          "if": {
            "properties": { "type": { "const": "networkx.NetworkXDataSet" } }
          },
          "then": {
            "required": ["filepath"],
            "properties": {
              "filepath": {
                "type": "string",
                "description": "The path to the NetworkX graph JSON file."
              },
              "load_args": {
                "type": "object",
                "description": "Arguments passed on to ```networkx.node_link_graph``.\nSee the details in\nhttps://networkx.org/documentation/networkx-1.9.1/reference/generated/networkx.readwrite.json_graph.node_link_graph.html"
              },
              "save_args": {
                "type": "object",
                "description": "Arguments passed on to ```networkx.node_link_data``.\nSee the details in\nhttps://networkx.org/documentation/networkx-1.9.1/reference/generated/networkx.readwrite.json_graph.node_link_data.html"
              },
              "credentials": {
                "type": "object",
                "description": "Credentials required to get access to the underlying filesystem.\nE.g. for ``GCSFileSystem`` it should look like `{\"token\": None}`."
              },
              "fs_args": {
                "type": "object",
                "description": "Extra arguments to pass into underlying filesystem class.\nE.g. for ``GCSFileSystem`` class: `{\"project\": \"my-project\", ...}`"
              },
              "layer": {
                "type": "string",
                "description": "The data layer according to the data engineering convention:\nhttps://kedro.readthedocs.io/en/stable/11_faq/01_faq.html#what-is-data-engineering-convention"
              }
            }
          }
        },
        {
          "if": {
            "properties": { "type": { "const": "dask.ParquetDataSet" } }
          },
          "then": {
            "required": ["filepath"],
            "properties": {
              "filepath": {
                "type": "string",
                "description": "Path to a parquet file\nparquet collection or the directory of a multipart parquet."
              },
              "load_args": {
                "type": "object",
                "description": "Additional loading options `dask.dataframe.read_parquet`:\nhttps://docs.dask.org/en/latest/dataframe-api.html#dask.dataframe.read_parquet"
              },
              "save_args": {
                "type": "object",
                "description": "Additional saving options for `dask.dataframe.to_parquet`:\nhttps://docs.dask.org/en/latest/dataframe-api.html#dask.dataframe.to_parquet"
              },
              "credentials": {
                "type": "object",
                "description": "Credentials required to get access to the underlying filesystem.\nE.g. for ``GCSFileSystem`` it should look like `{\"token\": None}`."
              },
              "fs_args": {
                "type": "object",
                "description": "Optional parameters to the backend file system driver:\nhttps://docs.dask.org/en/latest/remote-data-services.html#optional-parameters"
              },
              "layer": {
                "type": "string",
                "description": "The data layer according to the data engineering convention:\nhttps://kedro.readthedocs.io/en/stable/11_faq/01_faq.html#what-is-data-engineering-convention"
              }
            }
          }
        },
        {
          "if": {
            "properties": {
              "type": { "const": "biosequence.BioSequenceDataSet" }
            }
          },
          "then": {
            "required": ["filepath"],
            "properties": {
              "filepath": {
                "type": "string",
                "description": "path to sequence file prefixed with a protocol like `s3://`.\nIf prefix is not provided, `file` protocol (local filesystem) will be used.\nThe prefix should be any protocol supported by ``fsspec``."
              },
              "load_args": {
                "type": "object",
                "description": "Options for parsing sequence files by Biopython ``SeqIO.parse()``."
              },
              "save_args": {
                "type": "object",
                "description": "file format supported by Biopython ``SeqIO.write()``.\nE.g. `{\"format\": \"fasta\"}`."
              },
              "credentials": {
                "type": "object",
                "description": "Credentials required to get access to the underlying filesystem.\nE.g. for ``GCSFileSystem`` it should look like `{\"token\": None}`."
              },
              "fs_args": {
                "type": "object",
                "description": "Extra arguments to pass into underlying filesystem class.\nE.g. for ``GCSFileSystem`` class: `{\"project\": \"my-project\", ...}`."
              },
              "layer": {
                "type": "string",
                "description": "The data layer according to the data engineering convention:\nhttps://kedro.readthedocs.io/en/stable/11_faq/01_faq.html#what-is-data-engineering-convention\n\nNote: Here you can find all supported file formats: https://biopython.org/wiki/SeqIO"
              }
            }
          }
        },
        {
          "if": {
            "properties": { "type": { "const": "matplotlib.MatplotlibWriter" } }
          },
          "then": {
            "required": ["filepath"],
            "properties": {
              "filepath": {
                "type": "string",
                "description": "Key path to a matplot object file(s) prefixed with a protocol like `s3://`.\nIf prefix is not provided, `file` protocol (local filesystem) will be used.\nThe prefix should be any protocol supported by ``fsspec``."
              },
              "fs_args": {
                "type": "object",
                "description": "Extra arguments to pass into underlying filesystem class.\nE.g. for ``GCSFileSystem`` class: `{\"project\": \"my-project\", ...}`"
              },
              "credentials": {
                "type": "object",
                "description": "Credentials required to get access to the underlying filesystem.\nE.g. for ``S3FileSystem`` it should look like:\n`{'client_kwargs': {'aws_access_key_id': '<id>', 'aws_secret_access_key': '<key>'}}`"
              },
              "save_args": {
                "type": "object",
                "description": "Save args passed to `plt.savefig`. See\nhttps://matplotlib.org/api/_as_gen/matplotlib.pyplot.savefig.html"
              },
              "layer": {
                "type": "string",
                "description": "The data layer according to the data engineering convention:\nhttps://kedro.readthedocs.io/en/stable/11_faq/01_faq.html#what-is-data-engineering-convention"
              }
            }
          }
        },
        {
          "if": { "properties": { "type": { "const": "yaml.YAMLDataSet" } } },
          "then": {
            "required": ["filepath"],
            "properties": {
              "filepath": {
                "type": "string",
                "description": "Filepath to a YAML file prefixed with a protocol like `s3://`.\nIf prefix is not provided, `file` protocol (local filesystem) will be used.\nThe prefix should be any protocol supported by ``fsspec``.\nNote: `http(s)` doesn't support versioning."
              },
              "save_args": {
                "type": "object",
                "description": "PyYAML options for saving YAML files (arguments passed\ninto ```yaml.dump``). Here you can find all available arguments:\nhttps://pyyaml.org/wiki/PyYAMLDocumentation\nAll defaults are preserved, but \"default_flow_style\", which is set to False."
              },
              "credentials": {
                "type": "object",
                "description": "Credentials required to get access to the underlying filesystem.\nE.g. for ``GCSFileSystem`` it should look like `{\"token\": None}`."
              },
              "fs_args": {
                "type": "object",
                "description": "Extra arguments to pass into underlying filesystem class.\nE.g. for ``GCSFileSystem`` class: `{\"project\": \"my-project\", ...}`."
              },
              "layer": {
                "type": "string",
                "description": "The data layer according to the data engineering convention:\nhttps://kedro.readthedocs.io/en/stable/11_faq/01_faq.html#what-is-data-engineering-convention"
              }
            }
          }
        },
        {
          "if": {
            "properties": { "type": { "const": "pickle.PickleDataSet" } }
          },
          "then": {
            "required": ["filepath"],
            "properties": {
              "filepath": {
                "type": "string",
                "description": "Filepath to a Pickle file prefixed with a protocol like `s3://`.\nIf prefix is not provided, `file` protocol (local filesystem) will be used.\nThe prefix should be any protocol supported by ``fsspec``.\nNote: `http(s)` doesn't support versioning."
              },
              "load_args": {
                "type": "object",
                "description": "Pickle options for loading pickle files.\nHere you can find all available arguments:\nhttps://docs.python.org/3/library/pickle.html#pickle.loads\nAll defaults are preserved."
              },
              "save_args": {
                "type": "object",
                "description": "Pickle options for saving pickle files.\nHere you can find all available arguments:\nhttps://docs.python.org/3/library/pickle.html#pickle.dumps\nAll defaults are preserved."
              },
              "credentials": {
                "type": "object",
                "description": "Credentials required to get access to the underlying filesystem.\nE.g. for ``GCSFileSystem`` it should look like `{\"token\": None}`."
              },
              "fs_args": {
                "type": "object",
                "description": "Extra arguments to pass into underlying filesystem class.\nE.g. for ``GCSFileSystem`` class: `{\"project\": \"my-project\", ...}`."
              },
              "layer": {
                "type": "string",
                "description": "The data layer according to the data engineering convention:\nhttps://kedro.readthedocs.io/en/stable/11_faq/01_faq.html#what-is-data-engineering-convention"
              }
            }
          }
        },
        {
          "if": { "properties": { "type": { "const": "text.TextDataSet" } } },
          "then": {
            "required": ["filepath"],
            "properties": {
              "filepath": {
                "type": "string",
                "description": "Filepath to a text file prefixed with a protocol like `s3://`.\nIf prefix is not provided, `file` protocol (local filesystem) will be used.\nThe prefix should be any protocol supported by ``fsspec``.\nNote: `http(s)` doesn't support versioning."
              },
              "load_args": {
                "type": "object",
                "description": "Load arguments should be specified in accordance with\nthe open function of the underlying filesystem. E.g. for local file\nhttps://docs.python.org/3/library/functions.html#open"
              },
              "save_args": {
                "type": "object",
                "description": "Save arguments should be specified in accordance with\nthe open function of the underlying filesystem. E.g. for local file\nhttps://docs.python.org/3/library/functions.html#open"
              },
              "credentials": {
                "type": "object",
                "description": "Credentials required to get access to the underlying filesystem.\nE.g. for ``GCSFileSystem`` it should look like `{\"token\": None}`."
              },
              "fs_args": {
                "type": "object",
                "description": "Extra arguments to pass into underlying filesystem class.\nE.g. for ``GCSFileSystem`` class: `{\"project\": \"my-project\", ...}`."
              },
              "layer": {
                "type": "string",
                "description": "The data layer according to the data engineering convention:\nhttps://kedro.readthedocs.io/en/stable/11_faq/01_faq.html#what-is-data-engineering-convention"
              }
            }
          }
        },
        {
          "if": {
            "properties": { "type": { "const": "spark.SparkJDBCDataSet" } }
          },
          "then": {
            "required": ["url", "table"],
            "properties": {
              "url": {
                "type": "string",
                "description": "A JDBC URL of the form ``jdbc:subprotocol:subname``."
              },
              "table": {
                "type": "string",
                "description": "The name of the table to load or save data to."
              },
              "credentials": {
                "type": "object",
                "description": "A dictionary of JDBC database connection arguments.\nNormally at least properties ``user`` and ``password`` with\ntheir corresponding values.  It updates ``properties``\nparameter in ``load_args`` and ``save_args`` in case it is\nprovided."
              },
              "load_args": {
                "type": "object",
                "description": "Provided to underlying PySpark ``jdbc`` function along\nwith the JDBC URL and the name of the table. To find all\nsupported arguments, see here:\nhttps://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=jdbc#pyspark.sql.DataFrameReader.jdbc"
              },
              "save_args": {
                "type": "object",
                "description": "Provided to underlying PySpark ``jdbc`` function along\nwith the JDBC URL and the name of the table. To find all\nsupported arguments, see here:\nhttps://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=jdbc#pyspark.sql.DataFrameWriter.jdbc"
              },
              "layer": {
                "type": "string",
                "description": "The data layer according to the data engineering convention:\nhttps://kedro.readthedocs.io/en/stable/11_faq/01_faq.html#what-is-data-engineering-convention"
              }
            }
          }
        },
        {
          "if": {
            "properties": { "type": { "const": "spark.SparkHiveDataSet" } }
          },
          "then": {
            "required": ["database", "table", "write_mode"],
            "properties": {
              "database": {
                "type": "string",
                "description": "The name of the hive database."
              },
              "table": {
                "type": "string",
                "description": "The name of the table within the database."
              },
              "write_mode": {
                "type": "string",
                "description": "``insert``, ``upsert`` or ``overwrite`` are supported."
              },
              "table_pk": {
                "type": "array",
                "description": "If performing an upsert, this identifies the primary key columns used to\nresolve preexisting data. Is required for ``write_mode=\"upsert\"``."
              },
              "layer": {
                "type": "string",
                "description": "The data layer according to the data engineering convention:\nhttps://kedro.readthedocs.io/en/stable/11_faq/01_faq.html#what-is-data-engineering-convention"
              }
            }
          }
        },
        {
          "if": { "properties": { "type": { "const": "spark.SparkDataSet" } } },
          "then": {
            "required": ["filepath"],
            "properties": {
              "filepath": {
                "type": "string",
                "description": "Path to a Spark dataframe. When using Databricks\nand working with data written to mount path points,\nspecify ``filepath``s for (versioned) ``SparkDataSet``s\nstarting with ``/dbfs/mnt``."
              },
              "file_format": {
                "type": "string",
                "description": "File format used during load and save\noperations. These are formats supported by the running\nSparkContext include parquet, csv. For a list of supported\nformats please refer to Apache Spark documentation at\nhttps://spark.apache.org/docs/latest/sql-programming-guide.html"
              },
              "load_args": {
                "type": "object",
                "description": "Load args passed to Spark DataFrameReader load method.\nIt is dependent on the selected file format. You can find\na list of read options for each supported format\nin Spark DataFrame read documentation:\nhttps://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame"
              },
              "save_args": {
                "type": "object",
                "description": "Save args passed to Spark DataFrame write options.\nSimilar to load_args this is dependent on the selected file\nformat. You can pass ``mode`` and ``partitionBy`` to specify\nyour overwrite mode and partitioning respectively. You can find\na list of options for each format in Spark DataFrame\nwrite documentation:\nhttps://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame"
              },
              "credentials": {
                "type": "object",
                "description": "Credentials to access the S3 bucket, such as\n``aws_access_key_id``, ``aws_secret_access_key``, if ``filepath``\nprefix is ``s3a://`` or ``s3n://``. Optional keyword arguments passed to\n``hdfs.client.InsecureClient`` if ``filepath`` prefix is ``hdfs://``.\nIgnored otherwise."
              },
              "layer": {
                "type": "string",
                "description": "The data layer according to the data engineering convention:\nhttps://kedro.readthedocs.io/en/stable/11_faq/01_faq.html#what-is-data-engineering-convention"
              }
            }
          }
        },
        {
          "if": {
            "properties": { "type": { "const": "pandas.JSONBlobDataSet" } }
          },
          "then": {
            "required": ["filepath", "container_name", "credentials"],
            "properties": {
              "filepath": {
                "type": "string",
                "description": "Path to an Azure Blob of a JSON file."
              },
              "container_name": {
                "type": "string",
                "description": "Azure container name."
              },
              "credentials": {
                "type": "object",
                "description": "Credentials (``account_name`` and\n``account_key`` or ``sas_token``) to access the Azure Blob Storage."
              },
              "encoding": {
                "type": "string",
                "description": "Default utf-8. Defines encoding of JSON files downloaded as binary streams."
              },
              "blob_from_bytes_args": {
                "type": "object",
                "description": "Any additional arguments to pass to Azure's\n``create_blob_from_bytes`` method:\nhttps://docs.microsoft.com/en-us/python/api/azure.storage.blob.blockblobservice.blockblobservice?view=azure-python#create-blob-from-bytes"
              },
              "blob_to_bytes_args": {
                "type": "object",
                "description": "Any additional arguments to pass to Azure's\n``get_blob_to_bytes`` method:\nhttps://docs.microsoft.com/en-us/python/api/azure.storage.blob.baseblobservice.baseblobservice?view=azure-python#get-blob-to-bytes"
              },
              "load_args": {
                "type": "object",
                "description": "Pandas options for loading JSON files.\nHere you can find all available arguments:\nhttps://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_json.html\nAll defaults are preserved."
              },
              "save_args": {
                "type": "object",
                "description": "Pandas options for saving JSON files.\nHere you can find all available arguments:\nhttps://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_json.html\nAll defaults are preserved, but \"index\", which is set to False."
              },
              "layer": {
                "type": "string",
                "description": "The data layer according to the data engineering convention:\nhttps://kedro.readthedocs.io/en/stable/11_faq/01_faq.html#what-is-data-engineering-convention"
              }
            }
          }
        },
        {
          "if": { "properties": { "type": { "const": "pandas.JSONDataSet" } } },
          "then": {
            "required": ["filepath"],
            "properties": {
              "filepath": {
                "type": "string",
                "description": "Filepath to a JSON file prefixed with a protocol like `s3://`.\nIf prefix is not provided `file` protocol (local filesystem) will be used.\nThe prefix should be any protocol supported by ``fsspec``.\nNote: `http(s)` doesn't support versioning."
              },
              "load_args": {
                "type": "object",
                "description": "Pandas options for loading JSON files.\nHere you can find all available arguments:\nhttps://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_json.html\nAll defaults are preserved."
              },
              "save_args": {
                "type": "object",
                "description": "Pandas options for saving JSON files.\nHere you can find all available arguments:\nhttps://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_json.html\nAll defaults are preserved, but \"index\", which is set to False."
              },
              "credentials": {
                "type": "object",
                "description": "Credentials required to get access to the underlying filesystem.\nE.g. for ``GCSFileSystem`` it should look like `{'token': None}`."
              },
              "fs_args": {
                "type": "object",
                "description": "Extra arguments to pass into underlying filesystem class.\nE.g. for ``GCSFileSystem`` class: `{project: 'my-project', ...}`."
              },
              "layer": {
                "type": "string",
                "description": "The data layer according to the data engineering convention:\nhttps://kedro.readthedocs.io/en/stable/11_faq/01_faq.html#what-is-data-engineering-convention"
              }
            }
          }
        },
        {
          "if": {
            "properties": { "type": { "const": "pandas.SQLTableDataSet" } }
          },
          "then": {
            "required": ["table_name", "credentials"],
            "properties": {
              "table_name": {
                "type": "string",
                "description": "The table name to load or save data to. It\noverwrites name in ``save_args`` and ``table_name``\nparameters in ``load_args``."
              },
              "credentials": {
                "type": "object",
                "description": "A dictionary with a ``SQLAlchemy`` connection string.\nUsers are supposed to provide the connection string 'con'\nthrough credentials. It overwrites `con` parameter in\n``load_args`` and ``save_args`` in case it is provided. To find\nall supported connection string formats, see here:\nhttps://docs.sqlalchemy.org/en/13/core/engines.html#database-urls"
              },
              "load_args": {
                "type": "object",
                "description": "Provided to underlying pandas ``read_sql_table``\nfunction along with the connection string.\nTo find all supported arguments, see here:\nhttps://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html\nTo find all supported connection string formats, see here:\nhttps://docs.sqlalchemy.org/en/13/core/engines.html#database-urls"
              },
              "save_args": {
                "type": "object",
                "description": "Provided to underlying pandas ``to_sql`` function along\nwith the connection string.\nTo find all supported arguments, see here:\nhttps://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_sql.html\nTo find all supported connection string formats, see here:\nhttps://docs.sqlalchemy.org/en/13/core/engines.html#database-urls\nIt has ``index=False`` in the default parameters."
              },
              "layer": {
                "type": "string",
                "description": "The data layer according to the data engineering convention:\nhttps://kedro.readthedocs.io/en/stable/11_faq/01_faq.html#what-is-data-engineering-convention"
              }
            }
          }
        },
        {
          "if": {
            "properties": { "type": { "const": "pandas.SQLQueryDataSet" } }
          },
          "then": {
            "required": ["sql", "credentials"],
            "properties": {
              "sql": {
                "type": "string",
                "description": "The sql query statement."
              },
              "credentials": {
                "type": "object",
                "description": "A dictionary with a ``SQLAlchemy`` connection string.\nUsers are supposed to provide the connection string 'con'\nthrough credentials. It overwrites `con` parameter in\n``load_args`` and ``save_args`` in case it is provided. To find\nall supported connection string formats, see here:\nhttps://docs.sqlalchemy.org/en/13/core/engines.html#database-urls"
              },
              "load_args": {
                "type": "object",
                "description": "Provided to underlying pandas ``read_sql_query``\nfunction along with the connection string.\nTo find all supported arguments, see here:\nhttps://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_query.html\nTo find all supported connection string formats, see here:\nhttps://docs.sqlalchemy.org/en/13/core/engines.html#database-urls"
              },
              "layer": {
                "type": "string",
                "description": "The data layer according to the data engineering convention:\nhttps://kedro.readthedocs.io/en/stable/11_faq/01_faq.html#what-is-data-engineering-convention"
              }
            }
          }
        },
        {
          "if": {
            "properties": { "type": { "const": "pandas.ParquetDataSet" } }
          },
          "then": {
            "required": ["filepath"],
            "properties": {
              "filepath": {
                "type": "string",
                "description": "Filepath to a Parquet file prefixed with a protocol like `s3://`.\nIf prefix is not provided, `file` protocol (local filesystem) will be used.\nThe prefix should be any protocol supported by ``fsspec``.\nNote: `http(s)` doesn't support versioning."
              },
              "load_args": {
                "type": "object",
                "description": "Pandas options for loading Parquet files.\nHere you can find all available arguments:\nhttps://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_parquet.html\nAll defaults are preserved."
              },
              "save_args": {
                "type": "object",
                "description": "Additional saving options for `pyarrow.parquet.write_table`.\nHere you can find all available arguments:\nhttps://arrow.apache.org/docs/python/generated/pyarrow.parquet.write_table.html?highlight=write_table#pyarrow.parquet.write_table"
              },
              "credentials": {
                "type": "object",
                "description": "Credentials required to get access to the underlying filesystem.\nE.g. for ``GCSFileSystem`` it should look like `{\"token\": None}`."
              },
              "fs_args": {
                "type": "object",
                "description": "Extra arguments to pass into underlying filesystem class.\nE.g. for ``GCSFileSystem`` class: `{\"project\": \"my-project\", ...}`."
              },
              "layer": {
                "type": "string",
                "description": "The data layer according to the data engineering convention:\nhttps://kedro.readthedocs.io/en/stable/11_faq/01_faq.html#what-is-data-engineering-convention"
              }
            }
          }
        },
        {
          "if": {
            "properties": { "type": { "const": "pandas.FeatherDataSet" } }
          },
          "then": {
            "required": ["filepath"],
            "properties": {
              "filepath": {
                "type": "string",
                "description": "Filepath to a feather file prefixed with a protocol like `s3://`.\nIf prefix is not provided, `file` protocol (local filesystem) will be used.\nThe prefix should be any protocol supported by ``fsspec``.\nNote: `http(s)` doesn't support versioning."
              },
              "load_args": {
                "type": "object",
                "description": "Pandas options for loading feather files.\nHere you can find all available arguments:\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_feather.html\nAll defaults are preserved."
              },
              "credentials": {
                "type": "object",
                "description": "Credentials required to get access to the underlying filesystem.\nE.g. for ``GCSFileSystem`` it should look like `{\"token\": None}`."
              },
              "fs_args": {
                "type": "object",
                "description": "Extra arguments to pass into underlying filesystem class.\nE.g. for ``GCSFileSystem`` class: `{\"project\": \"my-project\", ...}`."
              },
              "layer": {
                "type": "string",
                "description": "The data layer according to the data engineering convention:\nhttps://kedro.readthedocs.io/en/stable/11_faq/01_faq.html#what-is-data-engineering-convention"
              }
            }
          }
        },
        {
          "if": {
            "properties": { "type": { "const": "pandas.CSVBlobDataSet" } }
          },
          "then": {
            "required": ["filepath", "container_name", "credentials"],
            "properties": {
              "filepath": {
                "type": "string",
                "description": "Path to an Azure Blob of a CSV file."
              },
              "container_name": {
                "type": "string",
                "description": "Azure container name."
              },
              "credentials": {
                "type": "object",
                "description": "Credentials (``account_name`` and\n``account_key`` or ``sas_token``) to access the Azure Blob Storage."
              },
              "blob_to_text_args": {
                "type": "object",
                "description": "Any additional arguments to pass to Azure's\n``get_blob_to_text`` method:\nhttps://docs.microsoft.com/en-us/python/api/azure-storage-blob/azure.storage.blob.baseblobservice.baseblobservice?view=azure-python#get-blob-to-text"
              },
              "blob_from_text_args": {
                "type": "object",
                "description": "Any additional arguments to pass to Azure's\n``create_blob_from_text`` method:\nhttps://docs.microsoft.com/en-us/python/api/azure-storage-blob/azure.storage.blob.baseblobservice.baseblobservice?view=azure-python#get-blob-to-text"
              },
              "load_args": {
                "type": "object",
                "description": "Pandas options for loading CSV files.\nHere you can find all available arguments:\nhttps://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\nAll defaults are preserved."
              },
              "save_args": {
                "type": "object",
                "description": "Pandas options for saving CSV files.\nHere you can find all available arguments:\nhttps://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html\nAll defaults are preserved, but \"index\", which is set to False."
              },
              "layer": {
                "type": "string",
                "description": "The data layer according to the data engineering convention:\nhttps://kedro.readthedocs.io/en/stable/11_faq/01_faq.html#what-is-data-engineering-convention"
              }
            }
          }
        },
        {
          "if": { "properties": { "type": { "const": "pandas.HDFDataSet" } } },
          "then": {
            "required": ["filepath", "key"],
            "properties": {
              "filepath": {
                "type": "string",
                "description": "Filepath to a hdf file prefixed with a protocol like `s3://`.\nIf prefix is not provided, `file` protocol (local filesystem) will be used.\nThe prefix should be any protocol supported by ``fsspec``.\nNote: `http(s)` doesn't support versioning."
              },
              "key": {
                "type": "string",
                "description": "Identifier to the group in the HDF store."
              },
              "load_args": {
                "type": "object",
                "description": "PyTables options for loading hdf files.\nYou can find all available arguments at:\nhttps://www.pytables.org/usersguide/libref/top_level.html#tables.open_file\nAll defaults are preserved."
              },
              "save_args": {
                "type": "object",
                "description": "PyTables options for saving hdf files.\nYou can find all available arguments at:\nhttps://www.pytables.org/usersguide/libref/top_level.html#tables.open_file\nAll defaults are preserved."
              },
              "credentials": {
                "type": "object",
                "description": "Credentials required to get access to the underlying filesystem.\nE.g. for ``GCSFileSystem`` it should look like `{\"token\": None}`."
              },
              "fs_args": {
                "type": "object",
                "description": "Extra arguments to pass into underlying filesystem class.\nE.g. for ``GCSFileSystem`` class: `{\"project\": \"my-project\", ...}`"
              },
              "layer": {
                "type": "string",
                "description": "The data layer according to the data engineering convention:\nhttps://kedro.readthedocs.io/en/stable/11_faq/01_faq.html#what-is-data-engineering-convention"
              }
            }
          }
        },
        {
          "if": { "properties": { "type": { "const": "pandas.CSVDataSet" } } },
          "then": {
            "required": ["filepath"],
            "properties": {
              "filepath": {
                "type": "string",
                "description": "Filepath to a CSV file prefixed with a protocol like `s3://`.\nIf prefix is not provided, `file` protocol (local filesystem) will be used.\nThe prefix should be any protocol supported by ``fsspec``.\nNote: `http(s)` doesn't support versioning."
              },
              "load_args": {
                "type": "object",
                "description": "Pandas options for loading CSV files.\nHere you can find all available arguments:\nhttps://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\nAll defaults are preserved."
              },
              "save_args": {
                "type": "object",
                "description": "Pandas options for saving CSV files.\nHere you can find all available arguments:\nhttps://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html\nAll defaults are preserved, but \"index\", which is set to False."
              },
              "credentials": {
                "type": "object",
                "description": "Credentials required to get access to the underlying filesystem.\nE.g. for ``GCSFileSystem`` it should look like `{\"token\": None}`."
              },
              "fs_args": {
                "type": "object",
                "description": "Extra arguments to pass into underlying filesystem class.\nE.g. for ``GCSFileSystem`` class: `{\"project\": \"my-project\", ...}`."
              },
              "layer": {
                "type": "string",
                "description": "The data layer according to the data engineering convention:\nhttps://kedro.readthedocs.io/en/stable/11_faq/01_faq.html#what-is-data-engineering-convention"
              }
            }
          }
        },
        {
          "if": {
            "properties": { "type": { "const": "pandas.ExcelDataSet" } }
          },
          "then": {
            "required": ["filepath"],
            "properties": {
              "filepath": {
                "type": "string",
                "description": "Filepath to a Excel file prefixed with a protocol like `s3://`.\nIf prefix is not provided, `file` protocol (local filesystem) will be used.\nThe prefix should be any protocol supported by ``fsspec``.\nNote: `http(s)` doesn't support versioning."
              },
              "engine": {
                "type": "string",
                "description": "The engine used to write to excel files. The default\nengine is 'xlsxwriter'."
              },
              "load_args": {
                "type": "object",
                "description": "Pandas options for loading Excel files.\nHere you can find all available arguments:\nhttps://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_excel.html\nAll defaults are preserved, but \"engine\", which is set to \"xlrd\"."
              },
              "save_args": {
                "type": "object",
                "description": "Pandas options for saving Excel files.\nHere you can find all available arguments:\nhttps://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_excel.html\nAll defaults are preserved, but \"index\", which is set to False.\nIf you would like to specify options for the `ExcelWriter`,\nyou can include them under \"writer\" key. Here you can\nfind all available arguments:\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.ExcelWriter.html"
              },
              "credentials": {
                "type": "object",
                "description": "Credentials required to get access to the underlying filesystem.\nE.g. for ``GCSFileSystem`` it should look like `{\"token\": None}`."
              },
              "fs_args": {
                "type": "object",
                "description": "Extra arguments to pass into underlying filesystem class.\nE.g. for ``GCSFileSystem`` class: `{\"project\": \"my-project\", ...}`."
              },
              "layer": {
                "type": "string",
                "description": "The data layer according to the data engineering convention:\nhttps://kedro.readthedocs.io/en/stable/11_faq/01_faq.html#what-is-data-engineering-convention"
              }
            }
          }
        },
        {
          "if": {
            "properties": { "type": { "const": "pandas.GBQTableDataSet" } }
          },
          "then": {
            "required": ["dataset", "table_name"],
            "properties": {
              "dataset": {
                "type": "string",
                "description": "Google BigQuery dataset."
              },
              "table_name": {
                "type": "string",
                "description": "Google BigQuery table name."
              },
              "project": {
                "type": "string",
                "description": "Google BigQuery Account project ID.\nOptional when available from the environment.\nhttps://cloud.google.com/resource-manager/docs/creating-managing-projects"
              },
              "credentials": {
                "pattern": ".*",
                "description": "Credentials for accessing Google APIs.\nEither ``google.auth.credentials.Credentials`` object or dictionary with\nparameters required to instantiate ``google.oauth2.credentials.Credentials``.\nHere you can find all the arguments:\nhttps://google-auth.readthedocs.io/en/latest/reference/google.oauth2.credentials.html"
              },
              "load_args": {
                "type": "object",
                "description": "Pandas options for loading BigQuery table into DataFrame.\nHere you can find all available arguments:\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_gbq.html\nAll defaults are preserved."
              },
              "save_args": {
                "type": "object",
                "description": "Pandas options for saving DataFrame to BigQuery table.\nHere you can find all available arguments:\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_gbq.html\nAll defaults are preserved, but \"progress_bar\", which is set to False."
              },
              "layer": {
                "type": "string",
                "description": "The data layer according to the data engineering convention:\nhttps://kedro.readthedocs.io/en/stable/11_faq/01_faq.html#what-is-data-engineering-convention"
              }
            }
          }
        },
        {
          "if": { "properties": { "type": { "const": "PickleLocalDataSet" } } },
          "then": {
            "required": ["filepath"],
            "properties": {
              "filepath": {
                "type": "string",
                "description": "path to a pkl file."
              },
              "backend": {
                "type": "string",
                "description": "backend to use, must be one of ['pickle', 'joblib']."
              },
              "load_args": {
                "type": "object",
                "description": "Options for loading pickle files. Refer to the help\nfile of ``pickle.load`` or ``joblib.load`` for options."
              },
              "save_args": {
                "type": "object",
                "description": "Options for saving pickle files. Refer to the help\nfile of ``pickle.dump`` or ``joblib.dump`` for options."
              }
            }
          }
        },
        {
          "if": { "properties": { "type": { "const": "JSONLocalDataSet" } } },
          "then": {
            "required": ["filepath"],
            "properties": {
              "filepath": {
                "type": "string",
                "description": "path to a local json file."
              },
              "load_args": {
                "type": "object",
                "description": "Arguments passed on to ```json.load``.\nSee https://docs.python.org/3/library/json.html for details.\nAll defaults are preserved."
              },
              "save_args": {
                "type": "object",
                "description": "Arguments passed on to ```json.dump``.\nSee https://docs.python.org/3/library/json.html\nfor details. All defaults are preserved."
              }
            }
          }
        },
        {
          "if": { "properties": { "type": { "const": "HDFLocalDataSet" } } },
          "then": {
            "required": ["filepath", "key"],
            "properties": {
              "filepath": {
                "type": "string",
                "description": "Path to an hdf file."
              },
              "key": {
                "type": "string",
                "description": "Identifier to the group in the HDF store."
              },
              "load_args": {
                "type": "object",
                "description": "Pandas options for loading hdf files.\nHere you can find all available arguments:\nhttps://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_hdf.html\nAll defaults are preserved."
              },
              "save_args": {
                "type": "object",
                "description": "Pandas options for saving hdf files.\nHere you can find all available arguments:\nhttps://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_hdf.html\nAll defaults are preserved."
              }
            }
          }
        },
        {
          "if": { "properties": { "type": { "const": "PartitionedDataSet" } } },
          "then": {
            "required": ["path", "dataset"],
            "properties": {
              "path": {
                "type": "string",
                "description": "Path to the folder containing partitioned data.\nIf path starts with the protocol (e.g., ``s3://``) then the\ncorresponding ``fsspec`` concrete filesystem implementation will\nbe used. If protocol is not specified,\n``fsspec.implementations.local.LocalFileSystem`` will be used.\n**Note:** Some concrete implementations are bundled with ``fsspec``,\nwhile others (like ``s3`` or ``gcs``) must be installed separately\nprior to usage of the ``PartitionedDataSet``."
              },
              "dataset": {
                "pattern": ".*",
                "description": "Underlying dataset definition. This is used to instantiate\nthe dataset for each file located inside the ``path``.\nAccepted formats are:\na) object of a class that inherits from ``AbstractDataSet``\nb) a string representing a fully qualified class name to such class\nc) a dictionary with ``type`` key pointing to a string from b),\nother keys are passed to the Dataset initializer.\nCredentials for the dataset can be explicitly specified in\nthis configuration."
              },
              "filepath_arg": {
                "type": "string",
                "description": "Underlying dataset initializer argument that will\ncontain a path to each corresponding partition file.\nIf unspecified, defaults to \"filepath\"."
              },
              "filename_suffix": {
                "type": "string",
                "description": "If specified, only partitions that end with this\nstring will be processed."
              },
              "credentials": {
                "type": "object",
                "description": "Protocol-specific options that will be passed to\n``fsspec.filesystem``\nhttps://filesystem-spec.readthedocs.io/en/latest/api.html#fsspec.filesystem\nand the dataset initializer. If the dataset config contains\nexplicit credentials spec, then such spec will take precedence.\n**Note:** ``dataset_credentials`` key has now been deprecated\nand should not be specified.\nAll possible credentials management scenarios are documented here:\nhttps://kedro.readthedocs.io/en/stable/04_user_guide/08_advanced_io.html#partitioned-dataset-credentials"
              },
              "load_args": {
                "type": "object",
                "description": "Keyword arguments to be passed into ``find()`` method of\nthe filesystem implementation."
              }
            }
          }
        },
        {
          "if": { "properties": { "type": { "const": "CachedDataSet" } } },
          "then": {
            "required": ["dataset"],
            "properties": {
              "dataset": {
                "pattern": ".*",
                "description": "A Kedro DataSet object or a dictionary to cache."
              },
              "copy_mode": {
                "type": "string",
                "description": "The copy mode used to copy the data. Possible\nvalues are: \"deepcopy\", \"copy\" and \"assign\". If not\nprovided, it is inferred based on the data type."
              }
            }
          }
        },
        {
          "if": { "properties": { "type": { "const": "JSONDataSet" } } },
          "then": {
            "required": ["filepath"],
            "properties": {
              "filepath": {
                "type": "string",
                "description": "Filepath to a JSON file prefixed with a protocol like `s3://`.\nIf prefix is not provided `file` protocol (local filesystem) will be used.\nThe prefix should be any protocol supported by ``fsspec``.\nNote: `http(s)` doesn't support versioning."
              },
              "load_args": {
                "type": "object",
                "description": "Pandas options for loading JSON files.\nHere you can find all available arguments:\nhttps://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_json.html\nAll defaults are preserved."
              },
              "save_args": {
                "type": "object",
                "description": "Pandas options for saving JSON files.\nHere you can find all available arguments:\nhttps://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_json.html\nAll defaults are preserved, but \"index\", which is set to False."
              },
              "credentials": {
                "type": "object",
                "description": "Credentials required to get access to the underlying filesystem.\nE.g. for ``GCSFileSystem`` it should look like `{'token': None}`."
              },
              "fs_args": {
                "type": "object",
                "description": "Extra arguments to pass into underlying filesystem class.\nE.g. for ``GCSFileSystem`` class: `{project: 'my-project', ...}`"
              }
            }
          }
        },
        {
          "if": { "properties": { "type": { "const": "CSVHTTPDataSet" } } },
          "then": {
            "required": ["fileurl"],
            "properties": {
              "fileurl": {
                "type": "string",
                "description": "A URL to fetch the CSV file."
              },
              "auth": {
                "pattern": ".*",
                "description": "Anything ``requests.get`` accepts. Normally it's either\n``('login', 'password')``, or ``AuthBase`` instance for more complex cases."
              },
              "load_args": {
                "pattern": ".*",
                "description": "Pandas options for loading csv files.\nHere you can find all available arguments:\nhttps://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\nAll defaults are preserved."
              }
            }
          }
        },
        {
          "if": { "properties": { "type": { "const": "MemoryDataSet" } } },
          "then": {
            "required": [],
            "properties": {
              "data": {
                "pattern": ".*",
                "description": "Python object containing the data."
              },
              "copy_mode": {
                "type": "string",
                "description": "The copy mode used to copy the data. Possible\nvalues are: \"deepcopy\", \"copy\" and \"assign\". If not\nprovided, it is inferred based on the data type."
              }
            }
          }
        },
        {
          "if": { "properties": { "type": { "const": "CSVLocalDataSet" } } },
          "then": {
            "required": ["filepath"],
            "properties": {
              "filepath": {
                "type": "string",
                "description": "path to a csv file."
              },
              "load_args": {
                "type": "object",
                "description": "Pandas options for loading csv files.\nHere you can find all available arguments:\nhttps://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\nAll defaults are preserved."
              },
              "save_args": {
                "type": "object",
                "description": "Pandas options for saving csv files.\nHere you can find all available arguments:\nhttps://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html\nAll defaults are preserved, but \"index\", which is set to False."
              }
            }
          }
        },
        {
          "if": { "properties": { "type": { "const": "ExcelLocalDataSet" } } },
          "then": {
            "required": ["filepath"],
            "properties": {
              "filepath": {
                "type": "string",
                "description": "path to an Excel file."
              },
              "engine": {
                "type": "string",
                "description": "The engine used to write to excel files. The default\nengine is 'xlsxwriter'."
              },
              "load_args": {
                "type": "object",
                "description": "Pandas options for loading Excel files. Here you can\nfind all available arguments:\nhttps://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_excel.html\nThe default_load_arg engine is 'xlrd', all others preserved."
              },
              "save_args": {
                "type": "object",
                "description": "Pandas options for saving Excel files. Here you can\nfind all available arguments:\nhttps://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_excel.html\nAll defaults are preserved, but \"index\", which is set to False.\nIf you would like to specify options for the `ExcelWriter`,\nyou can include them under \"writer\" key. Here you can\nfind all available arguments:\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.ExcelWriter.html"
              }
            }
          }
        },
        {
          "if": { "properties": { "type": { "const": "LambdaDataSet" } } },
          "then": {
            "required": ["load", "save"],
            "properties": {
              "load": {
                "pattern": ".*",
                "description": "Method to load data from a data set."
              },
              "save": {
                "pattern": ".*",
                "description": "Method to save data to a data set."
              },
              "exists": {
                "pattern": ".*",
                "description": "Method to check whether output data already exists."
              },
              "release": {
                "pattern": ".*",
                "description": "Method to release any cached information."
              }
            }
          }
        },
        {
          "if": { "properties": { "type": { "const": "HDFS3DataSet" } } },
          "then": {
            "required": ["filepath", "key"],
            "properties": {
              "filepath": {
                "type": "string",
                "description": "Path to an hdf file. May contain the full path in S3\nincluding bucket and protocol, e.g. `s3://bucket-name/path/to/file.hdf`."
              },
              "key": {
                "type": "string",
                "description": "Identifier to the group in the HDF store."
              },
              "bucket_name": {
                "type": "string",
                "description": "S3 bucket name. Must be specified **only** if not\npresent in ``filepath``."
              },
              "credentials": {
                "type": "object",
                "description": "Credentials to access the S3 bucket, such as\n``aws_access_key_id``, ``aws_secret_access_key``."
              },
              "load_args": {
                "type": "object",
                "description": "PyTables options for loading hdf files.\nYou can find all available arguments at:\nhttps://www.pytables.org/usersguide/libref/top_level.html#tables.open_file\nAll defaults are preserved."
              },
              "save_args": {
                "type": "object",
                "description": "PyTables options for saving hdf files.\nYou can find all available arguments at:\nhttps://www.pytables.org/usersguide/libref/top_level.html#tables.open_file\nAll defaults are preserved."
              },
              "s3fs_args": {
                "type": "object",
                "description": "S3FileSystem options. You can find all available arguments at:\nhttps://s3fs.readthedocs.io/en/latest/api.html#s3fs.core.S3FileSystem"
              }
            }
          }
        },
        {
          "if": { "properties": { "type": { "const": "PickleS3DataSet" } } },
          "then": {
            "required": ["filepath"],
            "properties": {
              "filepath": {
                "type": "string",
                "description": "path to a pkl file. May contain the full path in S3\nincluding bucket and protocol, e.g. `s3://bucket-name/path/to/file.pkl`."
              },
              "bucket_name": {
                "type": "string",
                "description": "S3 bucket name. Must be specified **only** if not\npresent in ``filepath``."
              },
              "credentials": {
                "type": "object",
                "description": "Credentials to access the S3 bucket, such as\n``aws_access_key_id``, ``aws_secret_access_key``."
              },
              "load_args": {
                "type": "object",
                "description": "Pickle options for loading pickle files.\nYou can find all available arguments at:\nhttps://docs.python.org/3/library/pickle.html#pickle.loads\nAll defaults are preserved."
              },
              "save_args": {
                "type": "object",
                "description": "Pickle options for saving pickle files.\nYou can see all available arguments at:\nhttps://docs.python.org/3/library/pickle.html#pickle.dumps\nAll defaults are preserved."
              },
              "s3fs_args": {
                "type": "object",
                "description": "S3FileSystem options. You can see all available arguments at:\nhttps://s3fs.readthedocs.io/en/latest/api.html#s3fs.core.S3FileSystem"
              }
            }
          }
        },
        {
          "if": { "properties": { "type": { "const": "SQLTableDataSet" } } },
          "then": {
            "required": ["table_name", "credentials"],
            "properties": {
              "table_name": {
                "type": "string",
                "description": "The table name to load or save data to. It\noverwrites name in ``save_args`` and ``table_name``\nparameters in ``load_args``."
              },
              "credentials": {
                "type": "object",
                "description": "A dictionary with a ``SQLAlchemy`` connection string.\nUsers are supposed to provide the connection string 'con'\nthrough credentials. It overwrites `con` parameter in\n``load_args`` and ``save_args`` in case it is provided. To find\nall supported connection string formats, see here:\nhttps://docs.sqlalchemy.org/en/13/core/engines.html#database-urls"
              },
              "load_args": {
                "type": "object",
                "description": "Provided to underlying pandas ``read_sql_table``\nfunction along with the connection string.\nTo find all supported arguments, see here:\nhttps://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html\nTo find all supported connection string formats, see here:\nhttps://docs.sqlalchemy.org/en/13/core/engines.html#database-urls"
              },
              "save_args": {
                "type": "object",
                "description": "Provided to underlying pandas ``to_sql`` function along\nwith the connection string.\nTo find all supported arguments, see here:\nhttps://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_sql.html\nTo find all supported connection string formats, see here:\nhttps://docs.sqlalchemy.org/en/13/core/engines.html#database-urls\nIt has ``index=False`` in the default parameters."
              }
            }
          }
        },
        {
          "if": { "properties": { "type": { "const": "SQLQueryDataSet" } } },
          "then": {
            "required": ["sql", "credentials"],
            "properties": {
              "sql": {
                "type": "string",
                "description": "The sql query statement."
              },
              "credentials": {
                "type": "object",
                "description": "A dictionary with a ``SQLAlchemy`` connection string.\nUsers are supposed to provide the connection string 'con'\nthrough credentials. It overwrites `con` parameter in\n``load_args`` and ``save_args`` in case it is provided. To find\nall supported connection string formats, see here:\nhttps://docs.sqlalchemy.org/en/13/core/engines.html#database-urls"
              },
              "load_args": {
                "type": "object",
                "description": "Provided to underlying pandas ``read_sql_query``\nfunction along with the connection string.\nTo find all supported arguments, see here:\nhttps://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_query.html\nTo find all supported connection string formats, see here:\nhttps://docs.sqlalchemy.org/en/13/core/engines.html#database-urls"
              }
            }
          }
        },
        {
          "if": { "properties": { "type": { "const": "CSVS3DataSet" } } },
          "then": {
            "required": ["filepath"],
            "properties": {
              "filepath": {
                "type": "string",
                "description": "Path to a csv file. May contain the full path in S3\nincluding bucket and protocol, e.g. `s3://bucket-name/path/to/file.csv`."
              },
              "bucket_name": {
                "type": "string",
                "description": "S3 bucket name. Must be specified **only** if not\npresent in ``filepath``."
              },
              "credentials": {
                "type": "object",
                "description": "Credentials to access the S3 bucket, such as\n``aws_access_key_id``, ``aws_secret_access_key``."
              },
              "load_args": {
                "type": "object",
                "description": "Pandas options for loading csv files.\nYou can find all available arguments at:\nhttps://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\nAll defaults are preserved."
              },
              "save_args": {
                "type": "object",
                "description": "Pandas options for saving csv files.\nYou can find all available arguments at:\nhttps://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html\nAll defaults are preserved, but \"index\", which is set to False."
              },
              "s3fs_args": {
                "type": "object",
                "description": "S3FileSystem options. You can see all available arguments at:\nhttps://s3fs.readthedocs.io/en/latest/api.html#s3fs.core.S3FileSystem"
              }
            }
          }
        },
        {
          "if": {
            "properties": { "type": { "const": "ParquetLocalDataSet" } }
          },
          "then": {
            "required": ["filepath"],
            "properties": {
              "filepath": {
                "type": "string",
                "description": "Path to a parquet file or a metadata file of a multipart\nparquet collection or the directory of a multipart parquet."
              },
              "engine": {
                "type": "string",
                "description": "The engine to use, one of: `auto`, `fastparquet`,\n`pyarrow`. If `auto`, then the default behavior is to try\n`pyarrow`, falling back to `fastparquet` if `pyarrow` is\nunavailable."
              },
              "load_args": {
                "type": "object",
                "description": "Additional loading options `pyarrow`:\nhttps://arrow.apache.org/docs/python/generated/pyarrow.parquet.read_table.html\nor `fastparquet`:\nhttps://fastparquet.readthedocs.io/en/latest/api.html#fastparquet.ParquetFile.to_pandas"
              },
              "save_args": {
                "type": "object",
                "description": "Additional saving options for `pyarrow`:\nhttps://arrow.apache.org/docs/python/generated/pyarrow.Table.html#pyarrow.Table.from_pandas\nor `fastparquet`:\nhttps://fastparquet.readthedocs.io/en/latest/api.html#fastparquet.write"
              }
            }
          }
        },
        {
          "if": { "properties": { "type": { "const": "TextLocalDataSet" } } },
          "then": {
            "required": ["filepath"],
            "properties": {
              "filepath": {
                "type": "string",
                "description": "path to a text file."
              },
              "load_args": {
                "type": "object",
                "description": "Load arguments should be specified in accordance with\nthe built in open function. This can be found at\nhttps://docs.python.org/3/library/functions.html#open"
              },
              "save_args": {
                "type": "object",
                "description": "Save arguments should be specified in accordance with\nthe built in open function. This can be found at\nhttps://docs.python.org/3/library/functions.html#open"
              }
            }
          }
        }
      ]
    }
  }
}
